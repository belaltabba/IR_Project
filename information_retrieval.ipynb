{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oPbFEprbgkiw"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import ir_datasets\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gZrAgfAaJIrH"
      },
      "outputs": [],
      "source": [
        "dataset1 = ir_datasets.load('antique/train')\n",
        "dataset2 = ir_datasets.load('lotte/lifestyle/dev/search')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gtxHU_QcjanF"
      },
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame(dataset1.docs_iter(), columns=['id', 'doc'])\n",
        "df1.to_csv('antique.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "df2 = pd.DataFrame(dataset2.docs_iter(), columns=['id', 'doc'])\n",
        "df2.to_csv('lotte.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('antique.csv')\n",
        "df2 = pd.read_csv('lotte.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "W41dgwYhSgYt"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "shortcut = {\n",
        "    'p.p.s':'post postscript',\n",
        "    'u.s.a': 'united states of america',\n",
        "    'a.k.a': 'also known as',\n",
        "    'm.a.d': 'Mutually Assured Destruction',\n",
        "    'a.b.b': 'Asea Brown Boveri',\n",
        "    's.c.o': 'Santa Cruz Operation',\n",
        "    'e.t.c': 'etcetera',\n",
        "    'm.i.t': 'Massachusetts Institute of Technology',\n",
        "    'v.i.p': 'very important person',\n",
        "    'us':'united states of america',\n",
        "    'u.s.':'united states of america',\n",
        "    'usa':'united states of america',\n",
        "    'cobol':'common business oriented language',\n",
        "    'rpm':'red hat package manager',\n",
        "    'ap':'associated press',\n",
        "    'gpa':'grade point average',\n",
        "    'npr':'national public radio',\n",
        "    'fema':'federal emergency',\n",
        "    'crt':'cathode ray tube',\n",
        "    'gm':'grandmaster',\n",
        "    'fps':'frames per second',\n",
        "    'pc':'personal computer',\n",
        "    'pms':'premenstrual syndrome',\n",
        "    'cia':'central intelligence agency',\n",
        "    'aids':'acquired immune deficiency syndrome',\n",
        "    'it\\'s':'it is',\n",
        "    'you\\'ve':'you have',\n",
        "    'what\\'s':'what is',\n",
        "    'that\\'s':'that is',\n",
        "    'who\\'s':'who is',\n",
        "    'don\\'t':'do not',\n",
        "    'haven\\'t':'have not',\n",
        "    'there\\'s':'there is',\n",
        "    'i\\'d':'i would',\n",
        "    'it\\'ll':'it will',\n",
        "    'i\\'m':'i am',\n",
        "    'here\\'s':'here is',\n",
        "    'you\\'ll':'you will',\n",
        "    'cant\\'t':'can not',\n",
        "    'didn\\'t':'did not',\n",
        "    'hadn\\'t':'had not',\n",
        "    'kv':'kilovolt',\n",
        "    'cc':'cubic centimeter',\n",
        "    'aoa':'american osteopathic association',\n",
        "    'rbi':'reserve bank',\n",
        "    'pls':'please',\n",
        "    'dvd':'digital versatile disc',\n",
        "    'bdu':'boise state university',\n",
        "    'dvd':'digital versatile disc',\n",
        "    'mac':'macintosh',\n",
        "    'tv':'television',\n",
        "    'cs':'computer science',\n",
        "    'cse':'computer science engineering',\n",
        "    'iit':'indian institutes of technology',\n",
        "    'uk':'united kingdom',\n",
        "    'eee':'electrical and electronics engineering',\n",
        "    'ca':'california',\n",
        "    'etc':'etcetera',\n",
        "    'ip':'internet protocol',\n",
        "    'bjp':'bharatiya janata party',\n",
        "    'gdp':' gross domestic product',\n",
        "    'un':'unitednations',\n",
        "    'ctc':'cost to company',\n",
        "    'atm':'automated teller machine',\n",
        "    'pvt':'private',\n",
        "    'iim':'indian institutes of management'\n",
        "    \n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def expand_contractions(text, shortcut):\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(re.escape(key) for key in shortcut.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = shortcut.get(match.lower())\n",
        "        expanded_contraction = first_char + expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    return expanded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vUbaiDPGgmJy"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XoUkFcwiSe_1"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "\n",
        "  text = expand_contractions(text, shortcut)\n",
        "\n",
        "  filtered_tokens = []\n",
        "  for token in word_tokenize(text):\n",
        "    token = re.sub(r'\\b[0-9]+\\b', '', token)\n",
        "    token = token.translate(str.maketrans('', '', string.punctuation))\n",
        "    token = token.lower()\n",
        "    if len(token) > 0 and token not in stopwords:\n",
        "      filtered_tokens.append(token)\n",
        "\n",
        "  # lemmatization\n",
        "  tagged_tokens = pos_tag(filtered_tokens)\n",
        "\n",
        "  # Lemmatize based on POS tags\n",
        "  lemmatized_words = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
        "  processed_text = ' '.join(lemmatized_words)\n",
        "  \n",
        "  return processed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1['doc'] = df1['doc'].apply(preprocess)\n",
        "\n",
        "for col in df1.columns:\n",
        "    if df1[col].dtype == 'float':\n",
        "        df1[col] = df1[col].astype(str)\n",
        "\n",
        "df1['doc'] = df1['doc'].fillna('')\n",
        "\n",
        "df1['doc'] = df1['doc'].astype(str)\n",
        "\n",
        "df1.to_csv('proccess_text.txt', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_object(obj, name):\n",
        "  with open(f'{name}.pkl', 'wb') as file:\n",
        "    pickle.dump(obj, file)\n",
        "\n",
        "def load_object(name):\n",
        "  with open(f'{name}.pkl', 'rb') as file:\n",
        "    obj = pickle.load(file)\n",
        "  return obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jFyJaknzV3Vl"
      },
      "outputs": [],
      "source": [
        "def create_tfidf_index(df):\n",
        "  vectorizer = TfidfVectorizer(preprocessor=preprocess)\n",
        "  df = df.dropna(subset=['doc'])\n",
        "  documents =  df['doc']\n",
        "  tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "  return tfidf_matrix, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKvMDCTJrDTQ"
      },
      "outputs": [],
      "source": [
        "tfidf_matrix_1, vectorizer_1 = create_tfidf_index(df1)\n",
        "save_object(tfidf_matrix_1, 'tfidf_matrix_1')\n",
        "save_object(vectorizer_1, 'vectorizer_1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5qv03i_g3bb"
      },
      "outputs": [],
      "source": [
        "tfidf_matrix_2, vectorizer_2 = create_tfidf_index(df2)\n",
        "save_object(tfidf_matrix_2, 'tfidf_matrix_2')\n",
        "save_object(vectorizer_2, 'vectorizer_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9kQ4LLCV6SRa"
      },
      "outputs": [],
      "source": [
        "tfidf_matrix_1 = load_object('tfidf_matrix_1')\n",
        "vectorizer_1 = load_object('vectorizer_1')\n",
        "tfidf_matrix_2 = load_object('tfidf_matrix_2')\n",
        "vectorizer_2 = load_object('vectorizer_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "suWNzX94bpVd"
      },
      "outputs": [],
      "source": [
        "query = 'I think Yuval is pretty spot on'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0BrucJ4-b5ky"
      },
      "outputs": [],
      "source": [
        "def search(query, dataset, tfidf_matrix, vectorizer, top_n=10):\n",
        "  normalized_query = preprocess(query)\n",
        "  query_vec = vectorizer.transform([normalized_query])\n",
        "  cosine_similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "  most_similar_docs_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
        "\n",
        "  results = [0] * top_n\n",
        "  docs_list = list(most_similar_docs_indices)\n",
        "  for i, doc in enumerate(dataset.docs_iter()):\n",
        "    if i in docs_list:\n",
        "      results[docs_list.index(i)] = doc.doc_id\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0Jf48zqT7pb",
        "outputId": "c12d7665-7cdf-4de1-84e6-64cf35efa05d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['2020338_3',\n",
              " '4220683_6',\n",
              " '3500408_6',\n",
              " '3908585_1',\n",
              " '1471424_3',\n",
              " '3821699_3',\n",
              " '623875_1',\n",
              " '273713_7',\n",
              " '2075216_1',\n",
              " '3211948_1']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search(query, dataset1, tfidf_matrix_1, vectorizer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCy7ReT3EGpw"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precission_at_10(relevant, retrieved):\n",
        "    num_relevant_retrieved = len(set(relevant).intersection(retrieved))\n",
        "    precision_at_10 = num_relevant_retrieved / 10\n",
        "    return precision_at_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recall_values(relevant, retrieved):\n",
        "    num_relevant_retrieved = len(relevant.intersection(retrieved))\n",
        "    num_relevant_total = len(relevant)\n",
        "    recall = num_relevant_retrieved / num_relevant_total\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_avg_precision(relevant, retrieved):\n",
        "     precision_sum = 0.0\n",
        "     num_relevant = len(relevant)\n",
        "     num_correct = 0\n",
        "     for i, doc in enumerate(retrieved):\n",
        "         if doc in relevant:\n",
        "             num_correct += 1\n",
        "             precision = num_correct / (i + 1)\n",
        "             precision_sum += precision\n",
        "\n",
        "     map  = precision_sum / num_relevant\n",
        "     return map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_reciprocal_rank(relevant, retrieved):\n",
        "     rr = 0\n",
        "     for i, doc in enumerate(retrieved):\n",
        "         if doc in relevant:\n",
        "             rr = 1/(i+1)\n",
        "             break\n",
        "     return rr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getRelevance1(query_id, qrels_new):\n",
        "    relevance1 = set()\n",
        "    for doc in qrels_new.get(query_id):\n",
        "        if(doc['relevance']==1):\n",
        "            relevance1.add(doc['doc_id'])\n",
        "    return relevance1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getRetrievedDocs(retrieved):\n",
        "    retrievedDocs = set()\n",
        "    for doc in retrieved:\n",
        "        retrievedDocs.add(doc['index'])\n",
        "    return retrievedDocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_evaluation(qrels_new):\n",
        "    AP = []\n",
        "    MRR = []\n",
        "    \n",
        "    for query in __doc__:\n",
        "        \n",
        "        relevance1 = getRelevance1(query.query_id, qrels_new)\n",
        "        retrieved = getRetrievedDocs(retrieved[query.query_id])\n",
        "        #recall\n",
        "        r = recall_values(relevance1, retrieved)\n",
        "        #precission @ 10\n",
        "        p = precission_at_10(relevance1, retrieved)\n",
        "        with open('evaluation.txt', 'a') as f:\n",
        "            f.write(f\"{query.query_id}: precision@k:{p:.3f} recall:{r:.3f}\\n\")\n",
        "        \n",
        "        map = mean_avg_precision(relevance1, retrieved)\n",
        "        AP.append(map)\n",
        "        \n",
        "        mrr = mean_reciprocal_rank(relevance1, retrieved)\n",
        "        MRR.append(mrr)\n",
        "    #MRR\n",
        "    mean_MRR = sum(MRR) / len(MRR)\n",
        "    #MAP\n",
        "    MAP = sum(AP) / len(AP)\n",
        "    with open('evaluation.txt', 'a') as f:\n",
        "        f.write(f\"{query.query_id}: MRR:{mean_MRR:.3f} MAP:{MAP:.3f}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NSkETFfG2hG4",
        "Y3hyQO14mt0N",
        "ODmoXZGTgTEV",
        "iu0W_wO_2qqQ",
        "8rTt825QV0VH",
        "iCy7ReT3EGpw",
        "uqiON5PQqiZ2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
